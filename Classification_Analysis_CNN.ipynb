{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evanjames19/LC_Pupil_Tracking-Cortical-State/blob/main/Classification_Analysis_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULmxVpDEDnsh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejN9W9oD18TR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "from scipy.stats import sem\n",
        "\n",
        "# Define file paths\n",
        "classify_table_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/control_new2.csv'\n",
        "\n",
        "# Define save paths\n",
        "save_path = '/content/drive/My Drive/autoencoder/NEW_DATA/PRO'\n",
        "\n",
        "# Ensure save paths exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Function to get pupil and eeg data, extracting the period 750:1750\n",
        "def get_pupil_eeg_data(data):\n",
        "    pupil_columns = [col for col in data.columns if col.startswith('RawPupil')]\n",
        "    eeg_columns = {\n",
        "        'DeltaPwr': [col for col in data.columns if col.startswith('DeltaPwr')],\n",
        "        'ThetaPwr': [col for col in data.columns if col.startswith('ThetaPwr')],\n",
        "        'AlphaPwr': [col for col in data.columns if col.startswith('AlphaPwr')],\n",
        "        'BetaPwr': [col for col in data.columns if col.startswith('BetaPwr')],\n",
        "        'LowGammaPwr': [col for col in data.columns if col.startswith('LowGammaPwr')],\n",
        "        'HighGammaPwr': [col for col in data.columns if col.startswith('HighGammaPwr')],\n",
        "    }\n",
        "\n",
        "    raw_pupil = data[pupil_columns].values[:, 1:1999]\n",
        "    eeg_bands = np.stack(\n",
        "        [data[eeg_columns[band]].values[:, 1:1999] for band in ['DeltaPwr', 'ThetaPwr', 'AlphaPwr', 'BetaPwr', 'LowGammaPwr', 'HighGammaPwr']],\n",
        "        axis=-1\n",
        "    )\n",
        "\n",
        "    return raw_pupil, eeg_bands\n",
        "\n",
        "# Function to process and save data\n",
        "def process_and_save_data(data, save_path):\n",
        "    animals = data['Animal'].unique()\n",
        "    all_data = {}\n",
        "\n",
        "    for animal in animals:\n",
        "        animal_data = data[data['Animal'] == animal]\n",
        "        raw_pupil, eeg_bands = get_pupil_eeg_data(animal_data)\n",
        "        stim_freq = animal_data['StimulationFrequency'].values\n",
        "\n",
        "        # Combine spon and stim data\n",
        "        labels = (stim_freq != 0).astype(int)\n",
        "\n",
        "        all_data[f'animal_{animal}'] = {\n",
        "            'pupil': raw_pupil,\n",
        "            'eeg': eeg_bands,\n",
        "            'label': labels\n",
        "        }\n",
        "\n",
        "    # Save the all_data dictionary\n",
        "    np.save(os.path.join(save_path, 'all_data.npy'), all_data)\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Process and save data\n",
        "all_data = process_and_save_data(pd.read_csv(classify_table_path), save_path)\n",
        "\n",
        "print(\"Data processed and saved successfully.\")\n",
        "\n",
        "# Load processed data\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# # Define the sets of animals\n",
        "# set1_animals = ['animal_24124', 'animal_33107', 'animal_3336', 'animal_3358', 'animal_3368']\n",
        "# set2_animals = ['animal_3398', 'animal_33105', 'animal_31117', 'animal_31108', 'animal_3734']\n",
        "set2_animals = ['animal_33118','animal_24124', 'animal_33135', 'animal_33119']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXeHalC3ObcP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from scipy.io import savemat\n",
        "\n",
        "# Define file paths\n",
        "classify_table_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/classify_table.csv'\n",
        "\n",
        "# Define save paths\n",
        "save_path = '/content/drive/My Drive/autoencoder/NEW_DATA/PRO'\n",
        "\n",
        "# Ensure save paths exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Function to get pupil and eeg data, extracting the period 750:1750\n",
        "def get_pupil_eeg_data(data):\n",
        "    pupil_columns = [col for col in data.columns if col.startswith('RawPupil')]\n",
        "    eeg_columns = {\n",
        "        'DeltaPwr': [col for col in data.columns if col.startswith('DeltaPwr')],\n",
        "        'ThetaPwr': [col for col in data.columns if col.startswith('ThetaPwr')],\n",
        "        'AlphaPwr': [col for col in data.columns if col.startswith('AlphaPwr')],\n",
        "        'BetaPwr': [col for col in data.columns if col.startswith('BetaPwr')],\n",
        "        'LowGammaPwr': [col for col in data.columns if col.startswith('LowGammaPwr')],\n",
        "        'HighGammaPwr': [col for col in data.columns if col.startswith('HighGammaPwr')],\n",
        "    }\n",
        "\n",
        "    raw_pupil = data[pupil_columns].values[:, 850:1850]\n",
        "    eeg_bands = np.stack(\n",
        "        [data[eeg_columns[band]].values[:, 850:1850] for band in ['DeltaPwr', 'ThetaPwr', 'AlphaPwr', 'BetaPwr', 'LowGammaPwr', 'HighGammaPwr']],\n",
        "        axis=-1\n",
        "    )\n",
        "\n",
        "    return raw_pupil, eeg_bands\n",
        "\n",
        "# Function to process and save data\n",
        "def process_and_save_data(data, save_path):\n",
        "    animals = data['Animal'].unique()\n",
        "    all_data = {}\n",
        "\n",
        "    for animal in animals:\n",
        "        animal_data = data[data['Animal'] == animal]\n",
        "        raw_pupil, eeg_bands = get_pupil_eeg_data(animal_data)\n",
        "        stim_freq = animal_data['StimulationFrequency'].values\n",
        "        session_ids = animal_data['Date'].values\n",
        "\n",
        "        # Combine spon and stim data\n",
        "        labels = (stim_freq != 0).astype(int)\n",
        "\n",
        "        all_data[f'animal_{animal}'] = {\n",
        "            'pupil': raw_pupil,\n",
        "            'eeg': eeg_bands,\n",
        "            'label': labels,\n",
        "            'session_id': session_ids\n",
        "        }\n",
        "\n",
        "    # Save the all_data dictionary\n",
        "    np.save(os.path.join(save_path, 'all_data.npy'), all_data)\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Process and save data\n",
        "all_data = process_and_save_data(pd.read_csv(classify_table_path), save_path)\n",
        "\n",
        "print(\"Data processed and saved successfully.\")\n",
        "\n",
        "# Load processed data\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Define the sets of animals\n",
        "set1_animals = ['animal_24124', 'animal_33107', 'animal_3336', 'animal_3358', 'animal_3368']\n",
        "set2_animals = ['animal_3398', 'animal_33105', 'animal_31117', 'animal_31108', 'animal_3734']\n",
        "\n",
        "# Extract and save session IDs for Set 2 animals\n",
        "for animal in set2_animals:\n",
        "    animal_data = all_data.get(animal)\n",
        "    if animal_data:\n",
        "        session_ids = animal_data['session_id']\n",
        "        mat_dict = {'session_ids': session_ids}\n",
        "        savemat(os.path.join(save_path, f'{animal}_session_ids.mat'), mat_dict)\n",
        "        print(f'Session IDs for {animal} saved successfully.')\n",
        "\n",
        "print(\"All session IDs for Set 2 animals saved as .mat files successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv-mX-w4EA-U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import sem, zscore\n",
        "\n",
        "# Load processed data\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Define the set of animals\n",
        "# set2_animals = ['animal_3398', 'animal_33105', 'animal_31117', 'animal_31108', 'animal_3734']\n",
        "set2_animals = [  'animal_33119', 'animal_24124', 'animal_33118']\n",
        "\n",
        "# Initialize lists to store the raw pupil data and EEG band data for spon and stim\n",
        "pupil_spon = []\n",
        "pupil_stim = []\n",
        "eeg_spon = {band: [] for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']}\n",
        "eeg_stim = {band: [] for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']}\n",
        "\n",
        "# Iterate over each animal in set2\n",
        "for animal_key in set2_animals:\n",
        "    animal_data = all_data[animal_key]\n",
        "    labels = animal_data['label']\n",
        "\n",
        "    # Separate the raw pupil data based on the labels\n",
        "    pupil_spon.append(animal_data['pupil'][labels == 0])\n",
        "    pupil_stim.append(animal_data['pupil'][labels == 1])\n",
        "\n",
        "    # Separate and z-score the EEG band data based on the labels\n",
        "    for i, band in enumerate(['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']):\n",
        "        eeg_spon[band].append(zscore(animal_data['eeg'][labels == 0, :, i], axis=None))\n",
        "        eeg_stim[band].append(zscore(animal_data['eeg'][labels == 1, :, i], axis=None))\n",
        "\n",
        "# Function to calculate mean and SEM\n",
        "def calculate_mean_sem(data):\n",
        "    data_concat = np.concatenate(data, axis=0)\n",
        "    mean = np.mean(data_concat, axis=0)\n",
        "    sem_value = sem(data_concat, axis=0)\n",
        "    return mean, sem_value\n",
        "\n",
        "# Calculate mean and SEM for raw pupil data\n",
        "pupil_spon_mean, pupil_spon_sem = calculate_mean_sem(pupil_spon)\n",
        "pupil_stim_mean, pupil_stim_sem = calculate_mean_sem(pupil_stim)\n",
        "\n",
        "# Plot Raw Pupil data\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(pupil_spon_mean, label='Spon', color='blue')\n",
        "plt.fill_between(range(len(pupil_spon_mean)), pupil_spon_mean - pupil_spon_sem, pupil_spon_mean + pupil_spon_sem, color='blue', alpha=0.3)\n",
        "plt.plot(pupil_stim_mean, label='Stim', color='red')\n",
        "plt.fill_between(range(len(pupil_stim_mean)), pupil_stim_mean - pupil_stim_sem, pupil_stim_mean + pupil_stim_sem, color='red', alpha=0.3)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Raw Pupil Size')\n",
        "plt.title('Average Raw Pupil Size with SEM')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate mean and SEM for each EEG band\n",
        "for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']:\n",
        "    eeg_spon_mean, eeg_spon_sem = calculate_mean_sem(eeg_spon[band])\n",
        "    eeg_stim_mean, eeg_stim_sem = calculate_mean_sem(eeg_stim[band])\n",
        "\n",
        "    # Plot EEG band data\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(eeg_spon_mean, label='Spon', color='blue')\n",
        "    plt.fill_between(range(len(eeg_spon_mean)), eeg_spon_mean - eeg_spon_sem, eeg_spon_mean + eeg_spon_sem, color='blue', alpha=0.3)\n",
        "    plt.plot(eeg_stim_mean, label='Stim', color='red')\n",
        "    plt.fill_between(range(len(eeg_stim_mean)), eeg_stim_mean - eeg_stim_sem, eeg_stim_mean + eeg_stim_sem, color='red', alpha=0.3)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel(f'{band} Power (z-scored)')\n",
        "    plt.title(f'Average {band} Power with SEM (z-scored)')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOrC4MMFDb2d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import sem, zscore\n",
        "\n",
        "# Load processed data\n",
        "save_path = '/content/drive/My Drive/autoencoder/NEW_DATA/update'\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Define the set of animals\n",
        "set2_animals = ['animal_3398', 'animal_33105', 'animal_31117', 'animal_31108', 'animal_3734']\n",
        "other_animals = ['animal_33117', 'animal_24116', 'animal_24124', 'animal_3335', 'animal_33119']\n",
        "\n",
        "# Initialize lists to store the raw pupil data and EEG band data for spon and stim\n",
        "pupil_spon = []\n",
        "pupil_stim = []\n",
        "eeg_spon = {band: [] for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']}\n",
        "eeg_stim = {band: [] for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']}\n",
        "\n",
        "# Iterate over all animals to separate spon and stim data\n",
        "for animal_key in all_data.keys():\n",
        "    animal_data = all_data[animal_key]\n",
        "    labels = animal_data['label']\n",
        "\n",
        "    if animal_key in set2_animals:\n",
        "        # Append stimulation data\n",
        "        pupil_stim.append(animal_data['pupil'][labels == 1])\n",
        "        for i, band in enumerate(['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']):\n",
        "            eeg_stim[band].append(zscore(animal_data['eeg'][labels == 1, :, i], axis=None))\n",
        "    elif animal_key in other_animals:\n",
        "        # Append spontaneous data\n",
        "        pupil_spon.append(animal_data['pupil'][labels == 0])\n",
        "        for i, band in enumerate(['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']):\n",
        "            eeg_spon[band].append(zscore(animal_data['eeg'][labels == 0, :, i], axis=None))\n",
        "\n",
        "# Function to calculate mean and SEM\n",
        "def calculate_mean_sem(data):\n",
        "    data_concat = np.concatenate(data, axis=0)\n",
        "    mean = np.mean(data_concat, axis=0)\n",
        "    sem_value = sem(data_concat, axis=0)\n",
        "    return mean, sem_value\n",
        "\n",
        "# Calculate mean and SEM for raw pupil data\n",
        "pupil_spon_mean, pupil_spon_sem = calculate_mean_sem(pupil_spon)\n",
        "pupil_stim_mean, pupil_stim_sem = calculate_mean_sem(pupil_stim)\n",
        "\n",
        "# Plot Raw Pupil data\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(pupil_spon_mean, label='Spon', color='blue')\n",
        "plt.fill_between(range(len(pupil_spon_mean)), pupil_spon_mean - pupil_spon_sem, pupil_spon_mean + pupil_spon_sem, color='blue', alpha=0.3)\n",
        "plt.plot(pupil_stim_mean, label='Stim', color='red')\n",
        "plt.fill_between(range(len(pupil_stim_mean)), pupil_stim_mean - pupil_stim_sem, pupil_stim_mean + pupil_stim_sem, color='red', alpha=0.3)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Raw Pupil Size')\n",
        "plt.title('Average Raw Pupil Size with SEM')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate mean and SEM for each EEG band\n",
        "for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']:\n",
        "    eeg_spon_mean, eeg_spon_sem = calculate_mean_sem(eeg_spon[band])\n",
        "    eeg_stim_mean, eeg_stim_sem = calculate_mean_sem(eeg_stim[band])\n",
        "\n",
        "    # Plot EEG band data\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(eeg_spon_mean, label='Spon', color='blue')\n",
        "    plt.fill_between(range(len(eeg_spon_mean)), eeg_spon_mean - eeg_spon_sem, eeg_spon_mean + eeg_spon_sem, color='blue', alpha=0.3)\n",
        "    plt.plot(eeg_stim_mean, label='Stim', color='red')\n",
        "    plt.fill_between(range(len(eeg_stim_mean)), eeg_stim_mean - eeg_stim_sem, eeg_stim_mean + eeg_stim_sem, color='red', alpha=0.3)\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel(f'{band} Power (z-scored)')\n",
        "    plt.title(f'Average {band} Power with SEM (z-scored)')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzZUKz8vzYK6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import os\n",
        "\n",
        "# Load processed data\n",
        "save_path = '/content/drive/My Drive/autoencoder/NEW_DATA/update'\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Define the set of animals\n",
        "set2_animals = ['animal_3398', 'animal_33105', 'animal_31117', 'animal_31108', 'animal_3734']\n",
        "\n",
        "# Iterate over each animal in set2\n",
        "for animal_key in set2_animals:\n",
        "    animal_data = all_data[animal_key]\n",
        "    labels = animal_data['label']\n",
        "\n",
        "    # Combine spon and stim pupil data\n",
        "    pupil_data = np.concatenate((animal_data['pupil'][labels == 0], animal_data['pupil'][labels == 1]), axis=0)\n",
        "    pupil_labels = np.concatenate((np.zeros(np.sum(labels == 0)), np.ones(np.sum(labels == 1))), axis=0)\n",
        "\n",
        "    # Perform t-SNE on the combined pupil data\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    tsne_result = tsne.fit_transform(pupil_data)\n",
        "\n",
        "    # Create a scatter plot of the t-SNE results\n",
        "    plt.figure()\n",
        "    scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=pupil_labels, cmap='bwr', alpha=0.7)\n",
        "    plt.colorbar(scatter, ticks=[0, 1], label='Type of Dilation')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.title(f't-SNE of Pupil Data - {animal_key}')\n",
        "    plt.grid(True)\n",
        "    plt.legend(handles=scatter.legend_elements()[0], labels=['Spontaneous', 'Stimulus-induced'])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0N-8LcHRKgQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import scipy.io\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load processed data\n",
        "save_path = '/content/drive/My Drive/autoencoder/NEW_DATA/update'\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Define the animal\n",
        "animal_key = 'animal_3734'\n",
        "\n",
        "# Get the data for the specific animal\n",
        "animal_data = all_data[animal_key]\n",
        "labels = animal_data['label']\n",
        "\n",
        "# Combine spon and stim pupil data\n",
        "pupil_data = np.concatenate((animal_data['pupil'][labels == 0], animal_data['pupil'][labels == 1]), axis=0)\n",
        "pupil_labels = np.concatenate((np.zeros(np.sum(labels == 0)), np.ones(np.sum(labels == 1))), axis=0)\n",
        "\n",
        "# Perform t-SNE on the combined pupil data\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_result = tsne.fit_transform(pupil_data)\n",
        "\n",
        "# Save the t-SNE results to a .mat file\n",
        "tsne_data = {\n",
        "    'tsne_result': tsne_result,\n",
        "    'pupil_labels': pupil_labels\n",
        "}\n",
        "scipy.io.savemat(os.path.join(save_path, 'tsne_pupil_data_animal_3734.mat'), tsne_data)\n",
        "\n",
        "# Create a scatter plot of the t-SNE results\n",
        "plt.figure()\n",
        "scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=pupil_labels, cmap='bwr', alpha=0.7)\n",
        "plt.colorbar(scatter, ticks=[0, 1], label='Type of Dilation')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.title(f't-SNE of Pupil Data - {animal_key}')\n",
        "plt.grid(True)\n",
        "plt.legend(handles=scatter.legend_elements()[0], labels=['Spontaneous', 'Stimulus-induced'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T3R0bYi2Ag5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import zscore\n",
        "from itertools import combinations\n",
        "from scipy.io import savemat\n",
        "import json\n",
        "\n",
        "# Original convolutional model function\n",
        "def build_classification_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1_output = x  # Save the output of Conv1D layer\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "    x = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
        "    conv2_output = x  # Save the output of Conv1D layer\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "    x = tf.keras.layers.Conv1D(256, 3, activation='relu', padding='same')(x)\n",
        "    conv3_output = x  # Save the output of Conv1D layer\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    dense_output = x  # Save the output of Dense layer\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Also create a model that outputs the intermediate layers\n",
        "    intermediate_layer_model = tf.keras.Model(inputs=inputs, outputs=[conv1_output, conv2_output, conv3_output, dense_output])\n",
        "\n",
        "    return model, intermediate_layer_model\n",
        "\n",
        "# New RNN model function\n",
        "def build_rnn_classification_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.LSTM(128, return_sequences=True)(inputs)\n",
        "    lstm_output = x  # Save the output of the LSTM layer\n",
        "    x = tf.keras.layers.LSTM(64)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    dense_output = x  # Save the output of Dense layer\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Also create a model that outputs the intermediate layers\n",
        "    intermediate_layer_model = tf.keras.Model(inputs=inputs, outputs=[lstm_output, dense_output])\n",
        "\n",
        "    return model, intermediate_layer_model\n",
        "\n",
        "    # New RNN model function using GRU layers\n",
        "def build_gru_classification_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.GRU(128, return_sequences=True)(inputs)\n",
        "    gru_output = x  # Save the output of the GRU layer\n",
        "    x = tf.keras.layers.GRU(64)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    dense_output = x  # Save the output of Dense layer\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Also create a model that outputs the intermediate layers\n",
        "    intermediate_layer_model = tf.keras.Model(inputs=inputs, outputs=[gru_output, dense_output])\n",
        "\n",
        "    return model, intermediate_layer_model\n",
        "\n",
        "\n",
        "# def extract_and_save_latent_features(model, intermediate_model, data, save_path, prefix):\n",
        "#     # latent_features = intermediate_model.predict(data)\n",
        "#     # lstm_features, dense_features = latent_features\n",
        "\n",
        "#     # # Save the features as arrays in shape (nTrials x # latent points)\n",
        "#     # np.save(os.path.join(save_path, f'{prefix}_lstm_features.npy'), lstm_features)\n",
        "#     # np.save(os.path.join(save_path, f'{prefix}_dense_features.npy'), dense_features)\n",
        "\n",
        "#     return lstm_features, dense_features\n",
        "\n",
        "\n",
        "# Custom callback to print accuracy at the end of each epoch\n",
        "class PrintAccuracyCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        train_acc = logs.get('accuracy')\n",
        "        val_acc = logs.get('val_accuracy')\n",
        "        print(f'Epoch {epoch + 1}, Train Accuracy: {train_acc:.4f}, Val Accuracy: {val_acc:.4f}')\n",
        "\n",
        "def train_with_combinations(data, labels, animal_ids, k, m):\n",
        "    unique_animals = np.unique(animal_ids)\n",
        "    best_model = None\n",
        "    intermediate_model = None\n",
        "    max_val_accuracy = 0\n",
        "    history_best = None\n",
        "    all_preds = []\n",
        "    all_true_labels = []\n",
        "    total_accuracy = 0\n",
        "    total_combinations = 0\n",
        "    all_accuracies = []\n",
        "\n",
        "    for train_animals in combinations(unique_animals, k-1):\n",
        "        train_indices = np.where(np.isin(animal_ids, train_animals))[0]\n",
        "        test_indices = np.where(~np.isin(animal_ids, train_animals))[0]\n",
        "        X_train, y_train = data[train_indices], labels[train_indices]\n",
        "        X_val, y_val = data[test_indices], labels[test_indices]\n",
        "        kf = KFold(n_splits=m)\n",
        "\n",
        "        for fold_idx, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
        "            print(f\"Pretraining step on {train_animals} ... fold {fold_idx}\")\n",
        "            X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
        "            y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
        "            model, intermediate_model = build_classification_model((X_train_fold.shape[1], 1))\n",
        "           # model, intermediate_model = build_rnn_classification_model((X_train_fold.shape[1], X_train_fold.shape[2]))\n",
        "            early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "            print_accuracy_callback = PrintAccuracyCallback()\n",
        "            history = model.fit(X_train_fold, y_train_fold, batch_size=32, epochs=100, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping, print_accuracy_callback], verbose=1, shuffle=True)\n",
        "            val_accuracy = max(history.history['val_accuracy'])\n",
        "            all_accuracies.append(val_accuracy)\n",
        "            total_accuracy += val_accuracy\n",
        "            total_combinations += 1\n",
        "            if val_accuracy > max_val_accuracy:\n",
        "                max_val_accuracy = val_accuracy\n",
        "                best_model = model\n",
        "                history_best = history.history\n",
        "            preds = model.predict(X_val_fold)\n",
        "            all_preds.extend(preds)\n",
        "            all_true_labels.extend(y_val_fold)\n",
        "\n",
        "    average_accuracy = total_accuracy / total_combinations if total_combinations > 0 else 0\n",
        "    print(f\"Best model-0 found! Proceeding to fine-tuning ...\")\n",
        "    return best_model, intermediate_model, history_best, average_accuracy, all_accuracies, all_preds, all_true_labels\n",
        "\n",
        "def fine_tune_and_test(animal, best_model, intermediate_model, X_train, y_train, m, save_path):\n",
        "    kf = KFold(n_splits=m)\n",
        "    max_val_accuracy = 0\n",
        "    best_fold_history = None\n",
        "    all_preds = []\n",
        "    all_true_labels = []\n",
        "    X_val_combined = np.empty((0, X_train.shape[1], X_train.shape[2]))\n",
        "    y_val_combined = np.empty((0,))\n",
        "\n",
        "    for fold_idx, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
        "        print(f\"Fine-tuning step on {animal} ... fold {fold_idx}\")\n",
        "        X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
        "        y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
        "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "        print_accuracy_callback = PrintAccuracyCallback()\n",
        "        history = best_model.fit(X_train_fold, y_train_fold, batch_size=32, epochs=100, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping, print_accuracy_callback], verbose=1, shuffle=True)\n",
        "        val_accuracy = max(history.history['val_accuracy'])\n",
        "        if val_accuracy > max_val_accuracy:\n",
        "            max_val_accuracy = val_accuracy\n",
        "            best_fold_history = history.history\n",
        "        preds = best_model.predict(X_val_fold)\n",
        "        all_preds.extend(preds)\n",
        "        all_true_labels.extend(y_val_fold)\n",
        "\n",
        "        # Combine validation data for saving latent features\n",
        "        X_val_combined = np.concatenate((X_val_combined, X_val_fold))\n",
        "        y_val_combined = np.concatenate((y_val_combined, y_val_fold))\n",
        "\n",
        "    # Extract and save latent features for all validation trials combined\n",
        "    # lstm_features, dense_features = extract_and_save_latent_features(best_model, intermediate_model, X_val_combined, save_path, f'fine_tune_{animal}')\n",
        "\n",
        "    print(f\"Best model-x found!\")\n",
        "    return max_val_accuracy, best_fold_history, all_preds, all_true_labels\n",
        "\n",
        "\n",
        "def cross_validate_animals(data, labels, animal_ids, k, m0, m1, save_path):\n",
        "    unique_animals = np.unique(animal_ids)\n",
        "    results = {}\n",
        "    for animal in unique_animals:\n",
        "        print(f\"Processing animal {animal} as the test animal...\")\n",
        "        train_indices = np.where(animal_ids != animal)[0]\n",
        "        test_indices = np.where(animal_ids == animal)[0]\n",
        "        X_train, y_train = data[train_indices], labels[train_indices]\n",
        "        best_model, intermediate_model, history_best, avg_accuracy_step1, all_accuracies, y_preds_pretrain, true_labels_pretrain = train_with_combinations(X_train, y_train, animal_ids[train_indices], k, m0)\n",
        "        X_train_ft, y_train_ft = data[test_indices], labels[test_indices]\n",
        "        val_accuracy_step2, best_fold_history, y_preds_finetune, true_labels_finetune= fine_tune_and_test(animal, best_model, intermediate_model, X_train_ft, y_train_ft, m1, save_path)\n",
        "        # lstm_features, dense_features = latent_features\n",
        "\n",
        "        results[f'animal_{animal}'] = {\n",
        "            'History_BestMdl0': history_best,\n",
        "            'AverageAccuracy_BestMdl0': avg_accuracy_step1,\n",
        "            'AllAccuracies_BestMdl0': all_accuracies,\n",
        "            'History_BestMdlx': best_fold_history,\n",
        "            'AverageAccuracy_BestMdlx': val_accuracy_step2,\n",
        "            'Weights_BestMdlx': [w.tolist() for w in best_model.get_weights()],  # Convert weights to list\n",
        "            'Y_Preds_BestMdl0': y_preds_pretrain,\n",
        "            'True_Labels_BestMdl0': true_labels_pretrain,\n",
        "            'Y_Preds_BestMdlx': y_preds_finetune,\n",
        "            'True_Labels_BestMdlx': true_labels_finetune,\n",
        "            # 'LSTM_Features': lstm_features,\n",
        "            # 'Dense_Features': dense_features\n",
        "        }\n",
        "    return results\n",
        "\n",
        "\n",
        "# Function to train and cross-validate on each animal\n",
        "def cross_validate_animal_individual(data, labels, animal_ids, m):\n",
        "    unique_animals = np.unique(animal_ids)\n",
        "    results = {}\n",
        "    for animal in unique_animals:\n",
        "        print(f\"Processing animal {animal}...\")\n",
        "        animal_indices = np.where(animal_ids == animal)[0]\n",
        "        X, y = data[animal_indices], labels[animal_indices]\n",
        "\n",
        "        kf = KFold(n_splits=m)\n",
        "        animal_results = {'accuracy': [], 'y_preds': [], 'true_labels': []}\n",
        "\n",
        "        for fold_idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "            print(f\"Training fold {fold_idx + 1} for animal {animal}...\")\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            model, intermediate_model = build_classification_model((X_train.shape[1], 1))\n",
        "            # model, intermediate_model = build_rnn_classification_model((X_train.shape[1], X_train.shape[2]))\n",
        "            early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "            print_accuracy_callback = PrintAccuracyCallback()\n",
        "            history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping, print_accuracy_callback], verbose=1, shuffle=True)\n",
        "\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred = (y_pred > 0.5).astype(int)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            print(f\"Fold {fold_idx + 1}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            animal_results['accuracy'].append(accuracy)\n",
        "            animal_results['y_preds'].extend(y_pred)\n",
        "            animal_results['true_labels'].extend(y_test)\n",
        "\n",
        "        results[f'animal_{animal}'] = animal_results\n",
        "    return results\n",
        "\n",
        "\n",
        "# Function to z-score each animal's trials for each frequency band separately\n",
        "def zscore_eeg_data(data, animal_ids):\n",
        "    unique_animals = np.unique(animal_ids)\n",
        "    zscored_data = np.empty_like(data)\n",
        "    for animal in unique_animals:\n",
        "        for band in range(data.shape[2]):\n",
        "            animal_indices = np.where(animal_ids == animal)[0]\n",
        "            zscored_data[animal_indices, :, band] = zscore(data[animal_indices, :, band], axis=None)\n",
        "    return zscored_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csiYnr6PClKy"
      },
      "outputs": [],
      "source": [
        "# Run cross-validation and save results\n",
        "m = 5 # number of folds used in cross-validation\n",
        "\n",
        "# Define the bands\n",
        "bands = ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']\n",
        "\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Loop over each band\n",
        "for i, band in enumerate(bands):\n",
        "    data = np.concatenate([all_data[animal]['eeg'][..., i:i+1] for animal in set2_animals])\n",
        "    labels = np.concatenate([all_data[animal]['label'] for animal in set2_animals])\n",
        "    animal_ids = np.concatenate([[animal] * len(all_data[animal]['label']) for animal in set2_animals])\n",
        "\n",
        "    # Z-score the data for each animal and each frequency band separately\n",
        "    data = zscore_eeg_data(data, animal_ids)\n",
        "\n",
        "    # Perform cross-validation on each animal\n",
        "    results = cross_validate_animal_individual(data, labels, animal_ids, m)\n",
        "\n",
        "    # Save results\n",
        "    scipy.io.savemat(os.path.join(save_path, f'resultsall_{i}.mat'), {f'resultsall_{i}': results})\n",
        "\n",
        "    print(f\"Cross-validation for {band} band completed and results saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9IXf_RY4LguV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Run cross-validation and save results\n",
        "import scipy\n",
        "\n",
        "set2_animals = ['animal_33118', 'animal_33135','animal_24124']\n",
        "\n",
        "k = 3 # number of total animals\n",
        "m0 = 3 # number of folds used in pretraining\n",
        "m1 = 3 # number of folds used in fine-tuning\n",
        "\n",
        "# Define the bands\n",
        "bands = ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']\n",
        "\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Loop over each band\n",
        "for i, band in enumerate(bands):\n",
        "    data = np.concatenate([all_data[animal]['eeg'][..., i:i+1] for animal in set2_animals])\n",
        "    labels = np.concatenate([all_data[animal]['label'] for animal in set2_animals])\n",
        "    animal_ids = np.concatenate([[animal] * len(all_data[animal]['label']) for animal in set2_animals])\n",
        "\n",
        "    # Z-score the data for each animal and each frequency band separately\n",
        "    data = zscore_eeg_data(data, animal_ids)\n",
        "\n",
        "    # Perform cross-validation\n",
        "    results = cross_validate_animals(data, labels, animal_ids, k, m0, m1, save_path)\n",
        "\n",
        "    scipy.io.savemat(os.path.join(save_path, f'results_PROP__3_{i}.mat'), {f'results_PROP__3_{i}': results})\n",
        "\n",
        "    print(f\"Cross-validation for {band} band completed and results saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4VAlAG9MCo1"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = np.concatenate([all_data[animal]['eeg'] for animal in set2_animals])\n",
        "labels = np.concatenate([all_data[animal]['label'] for animal in set2_animals])\n",
        "animal_ids = np.concatenate([[animal] * len(all_data[animal]['label']) for animal in set2_animals])\n",
        "data = zscore_eeg_data(data, animal_ids)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to calculate and plot the average EEG band trace based on labels\n",
        "def plot_average_eeg_band_trace(data, labels, animal_ids, bands):\n",
        "    unique_animals = np.unique(animal_ids)\n",
        "    averaged_traces = {band: {0: [], 1: []} for band in bands}\n",
        "\n",
        "    for animal in unique_animals:\n",
        "        animal_indices = np.where(animal_ids == animal)[0]\n",
        "        animal_data = data[animal_indices]\n",
        "        animal_labels = labels[animal_indices]\n",
        "        for i, band in enumerate(bands):\n",
        "            band_data = animal_data[:, :, i]\n",
        "            spon_trials = band_data[animal_labels == 0]\n",
        "            stim_trials = band_data[animal_labels == 1]\n",
        "            averaged_traces[band][0].append(np.mean(spon_trials, axis=0))\n",
        "            averaged_traces[band][1].append(np.mean(stim_trials, axis=0))\n",
        "\n",
        "    for band in bands:\n",
        "        mean_trace_spon = np.mean(averaged_traces[band][0], axis=0)\n",
        "        sem_trace_spon = sem(averaged_traces[band][0], axis=0)\n",
        "        mean_trace_stim = np.mean(averaged_traces[band][1], axis=0)\n",
        "        sem_trace_stim = sem(averaged_traces[band][1], axis=0)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(mean_trace_spon, label=f'{band} Spontaneous Mean')\n",
        "        plt.fill_between(range(len(mean_trace_spon)), mean_trace_spon - sem_trace_spon, mean_trace_spon + sem_trace_spon, alpha=0.3, label=f'{band} Spontaneous SEM')\n",
        "        plt.plot(mean_trace_stim, label=f'{band} Stimulated Mean')\n",
        "        plt.fill_between(range(len(mean_trace_stim)), mean_trace_stim - sem_trace_stim, mean_trace_stim + sem_trace_stim, alpha=0.3, label=f'{band} Stimulated SEM')\n",
        "        plt.title(f'Average {band} Band Trace Across All Animals')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Z-scored Power')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# Define the EEG bands\n",
        "bands = ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']\n",
        "\n",
        "# Plot the average EEG band trace based on labels\n",
        "plot_average_eeg_band_trace(data, labels, animal_ids, bands)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb5xj7WmMtyQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import zscore\n",
        "from scipy.io import savemat\n",
        "\n",
        "# Function to build the classification model\n",
        "def build_classification_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1_output = x  # Save the output of Conv1D layer\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "    x = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
        "    conv2_output = x  # Save the output of Conv1D layer\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "    x = tf.keras.layers.Conv1D(256, 3, activation='relu', padding='same')(x)\n",
        "    conv3_output = x  # Save the output of Conv1D layer\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    dense_output = x  # Save the output of Dense layer\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Custom callback to print accuracy at the end of each epoch\n",
        "class PrintAccuracyCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        train_acc = logs.get('accuracy')\n",
        "        val_acc = logs.get('val_accuracy')\n",
        "        print(f'Epoch {epoch + 1}, Train Accuracy: {train_acc:.4f}, Val Accuracy: {val_acc:.4f}')\n",
        "\n",
        "# Function to z-score each animal's trials for each frequency band separately\n",
        "def zscore_eeg_data(data, animal_ids):\n",
        "    unique_animals = np.unique(animal_ids)\n",
        "    zscored_data = np.empty_like(data)\n",
        "    for animal in unique_animals:\n",
        "        animal_indices = np.where(animal_ids == animal)[0]\n",
        "        for band in range(data.shape[2]):\n",
        "            zscored_data[animal_indices, :, band] = zscore(data[animal_indices, :, band], axis=None)\n",
        "    return zscored_data\n",
        "\n",
        "# Function to train and cross-validate on all data\n",
        "def cross_validate_all_data(data, labels, m):\n",
        "    kf = KFold(n_splits=m, shuffle=True, random_state=42)\n",
        "    all_accuracies = []\n",
        "    all_preds = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    for fold_idx, (train_index, test_index) in enumerate(kf.split(data)):\n",
        "        print(f\"Training fold {fold_idx + 1}...\")\n",
        "        X_train, X_val = data[train_index], data[test_index]\n",
        "        y_train, y_val = labels[train_index], labels[test_index]\n",
        "\n",
        "        # Shuffle the training data\n",
        "        shuffle_idx = np.random.permutation(len(X_train))\n",
        "        X_train = X_train[shuffle_idx]\n",
        "        y_train = y_train[shuffle_idx]\n",
        "\n",
        "        model = build_classification_model((X_train.shape[1], X_train.shape[2]))\n",
        "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "        print_accuracy_callback = PrintAccuracyCallback()\n",
        "        history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping, print_accuracy_callback], verbose=1, shuffle=True)\n",
        "\n",
        "        val_accuracy = max(history.history['val_accuracy'])\n",
        "        all_accuracies.append(val_accuracy)\n",
        "\n",
        "        preds = model.predict(X_val)\n",
        "        all_preds.extend(preds)\n",
        "        all_true_labels.extend(y_val)\n",
        "\n",
        "    average_accuracy = np.mean(all_accuracies)\n",
        "    print(f\"Cross-validation completed. Average Validation Accuracy: {average_accuracy:.4f}\")\n",
        "    return average_accuracy, all_accuracies, all_preds, all_true_labels\n",
        "\n",
        "# Main code to load data, z-score, and perform cross-validation\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Define the set of animals\n",
        "# set2_animals = ['animal_3398', 'animal_33105', 'animal_31117', 'animal_31108', 'animal_3734']\n",
        "# other_animals = ['animal_33117', 'animal_24116', 'animal_24124', 'animal_3335', 'animal_33119']\n",
        "\n",
        "set2_animals = [ 'animal_33105', 'animal_31117']\n",
        "other_animals = [ 'animal_24124', 'animal_3335']\n",
        "\n",
        "set2_animals = ['animal_24124', 'animal_33118', 'animal_33135', 'animal_33119']\n",
        "# set2_animals = [ 'animal_33118', 'animal_33119', 'animal_33135']\n",
        "\n",
        "\n",
        "# Initialize lists to store the raw pupil data and EEG band data for spon and stim\n",
        "eeg_spon = {band: [] for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']}\n",
        "eeg_stim = {band: [] for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']}\n",
        "\n",
        "# Iterate over all animals to separate spon and stim data\n",
        "for animal_key in all_data.keys():\n",
        "    animal_data = all_data[animal_key]\n",
        "    labels = animal_data['label']\n",
        "\n",
        "    if animal_key in set2_animals:\n",
        "        # Append stimulation data\n",
        "        for i, band in enumerate(['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']):\n",
        "            eeg_stim[band].append(zscore(animal_data['eeg'][labels == 1, :, i], axis=None))\n",
        "            eeg_spon[band].append(zscore(animal_data['eeg'][labels == 0, :, i], axis=None))\n",
        "\n",
        "# Define the bands\n",
        "bands = ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']\n",
        "\n",
        "# Loop over each band\n",
        "for i, band in enumerate(bands):\n",
        "    print(f\"Processing {band} band...\")\n",
        "\n",
        "    combined_data = []\n",
        "    combined_labels = []\n",
        "    animal_ids = []\n",
        "\n",
        "    for animal_key in all_data.keys():\n",
        "        animal_data = all_data[animal_key]\n",
        "        labels = animal_data['label']\n",
        "        eeg_data = animal_data['eeg'][..., i:i+1]  # Select the current band data\n",
        "\n",
        "        if animal_key in set2_animals:\n",
        "            combined_data.append(eeg_data[labels == 1])\n",
        "            combined_labels.append(np.ones(np.sum(labels == 1)))\n",
        "            combined_data.append(eeg_data[labels == 0])\n",
        "            combined_labels.append(np.zeros(np.sum(labels == 0)))\n",
        "\n",
        "        animal_ids.extend([animal_key] * len(labels))\n",
        "\n",
        "    combined_data = np.concatenate(combined_data, axis=0)\n",
        "    combined_labels = np.concatenate(combined_labels, axis=0)\n",
        "    animal_ids = np.array(animal_ids[:combined_data.shape[0]])\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    print(f\"Shape of combined_data: {combined_data.shape}\")\n",
        "    print(f\"Shape of combined_labels: {combined_labels.shape}\")\n",
        "    print(f\"Shape of animal_ids: {animal_ids.shape}\")\n",
        "\n",
        "    # Z-score the data for each animal and each frequency band separately\n",
        "    combined_data = zscore_eeg_data(combined_data, animal_ids)\n",
        "\n",
        "    # Perform cross-validation\n",
        "    m = 5  # Number of folds\n",
        "    average_accuracy, all_accuracies, all_preds, all_true_labels = cross_validate_all_data(combined_data, combined_labels, m)\n",
        "\n",
        "    # Save results\n",
        "    results = {\n",
        "        'AverageAccuracy': average_accuracy,\n",
        "        'AllAccuracies': all_accuracies,\n",
        "        'Y_Preds': all_preds,\n",
        "        'True_Labels': all_true_labels\n",
        "    }\n",
        "\n",
        "    savemat(os.path.join(save_path, f'resultclon_{band}_band.mat'), results)\n",
        "    print(f\"Cross-validation for {band} band completed and results saved successfully.\")\n",
        "\n",
        "# # # Initialize lists to store the raw pupil data and EEG band data for spon and stim\n",
        "# eeg_spon = {band: [] for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']}\n",
        "# eeg_stim = {band: [] for band in ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']}\n",
        "\n",
        "# # Iterate over all animals to separate spon and stim data\n",
        "# for animal_key in all_data.keys():\n",
        "#     animal_data = all_data[animal_key]\n",
        "#     labels = animal_data['label']\n",
        "\n",
        "#     if animal_key in set2_animals:\n",
        "#         # Append stimulation data\n",
        "#         for i, band in enumerate(['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']):\n",
        "#             eeg_stim[band].append(zscore(animal_data['eeg'][labels == 1, :, i], axis=None))\n",
        "#             # eeg_spon[band].append(zscore(animal_data['eeg'][labels == 0, :, i], axis=None))\n",
        "\n",
        "#     elif animal_key in other_animals:\n",
        "#         # Append spontaneous data\n",
        "#         for i, band in enumerate(['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']):\n",
        "#             eeg_spon[band].append(zscore(animal_data['eeg'][labels == 0, :, i], axis=None))\n",
        "\n",
        "# # Define the bands\n",
        "# bands = ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']\n",
        "\n",
        "# # Loop over each band\n",
        "# for i, band in enumerate(bands):\n",
        "#     print(f\"Processing {band} band...\")\n",
        "\n",
        "#     combined_data = []\n",
        "#     combined_labels = []\n",
        "#     animal_ids = []\n",
        "\n",
        "#     for animal_key in all_data.keys():\n",
        "#         animal_data = all_data[animal_key]\n",
        "#         labels = animal_data['label']\n",
        "#         eeg_data = animal_data['eeg'][..., i:i+1]  # Select the current band data\n",
        "\n",
        "#         if animal_key in set2_animals:\n",
        "#             combined_data.append(eeg_data[labels == 1])\n",
        "#             combined_labels.append(np.ones(np.sum(labels == 1)))\n",
        "#             # combined_data.append(eeg_data[labels == 0])\n",
        "#             # combined_labels.append(np.zeros(np.sum(labels == 0)))\n",
        "\n",
        "#         elif animal_key in other_animals:\n",
        "#             combined_data.append(eeg_data[labels == 0])\n",
        "#             combined_labels.append(np.zeros(np.sum(labels == 0)))\n",
        "\n",
        "#         animal_ids.extend([animal_key] * len(labels))\n",
        "\n",
        "#     combined_data = np.concatenate(combined_data, axis=0)\n",
        "#     combined_labels = np.concatenate(combined_labels, axis=0)\n",
        "#     animal_ids = np.array(animal_ids[:combined_data.shape[0]])\n",
        "\n",
        "#     # Print shapes for debugging\n",
        "#     print(f\"Shape of combined_data: {combined_data.shape}\")\n",
        "#     print(f\"Shape of combined_labels: {combined_labels.shape}\")\n",
        "#     print(f\"Shape of animal_ids: {animal_ids.shape}\")\n",
        "\n",
        "#     # Z-score the data for each animal and each frequency band separately\n",
        "#     combined_data = zscore_eeg_data(combined_data, animal_ids)\n",
        "\n",
        "#     # Perform cross-validation\n",
        "#     m = 5  # Number of folds\n",
        "#     average_accuracy, all_accuracies, all_preds, all_true_labels = cross_validate_all_data(combined_data, combined_labels, m)\n",
        "\n",
        "#     # Save results\n",
        "#     results = {\n",
        "#         'AverageAccuracy': average_accuracy,\n",
        "#         'AllAccuracies': all_accuracies,\n",
        "#         'Y_Preds': all_preds,\n",
        "#         'True_Labels': all_true_labels\n",
        "#     }\n",
        "\n",
        "#     savemat(os.path.join(save_path, f'results_includes_combo1_{band}_band.mat'), results)\n",
        "#     print(f\"Cross-validation for {band} band completed and results saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJWKYSu-FcGB"
      },
      "outputs": [],
      "source": [
        "# Save eeg pupil data as matfile\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "\n",
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Define the set of animals\n",
        "set2_animals = ['animal_3398', 'animal_33105', 'animal_31117', 'animal_31108', 'animal_3734']\n",
        "other_animals = ['animal_33117', 'animal_24116', 'animal_24124', 'animal_3335', 'animal_33119']\n",
        "set2_animals = ['animal_33119', 'animal_33118', 'animal_33135']\n",
        "\n",
        "# Initialize lists to store the raw pupil data and EEG band data for spon and stim\n",
        "pupil_spon = []\n",
        "pupil_stim = []\n",
        "eeg_spon = []\n",
        "eeg_stim = []\n",
        "\n",
        "# Iterate over all animals to separate spon and stim data\n",
        "for animal_key in all_data.keys():\n",
        "    animal_data = all_data[animal_key]\n",
        "    labels = animal_data['label']\n",
        "    pupil_data = animal_data['pupil']\n",
        "    eeg_data = animal_data['eeg']\n",
        "\n",
        "    if animal_key in set2_animals:\n",
        "        # Append stimulation data\n",
        "        pupil_stim.append(pupil_data[labels == 1])\n",
        "        eeg_stim.append(eeg_data[labels == 1])\n",
        "        pupil_spon.append(pupil_data[labels == 0])\n",
        "        eeg_spon.append(eeg_data[labels == 0])\n",
        "    # elif animal_key in other_animals:\n",
        "    #     # Append spontaneous data\n",
        "    #     pupil_spon.append(pupil_data[labels == 0])\n",
        "    #     eeg_spon.append(eeg_data[labels == 0])\n",
        "\n",
        "# Combine data for each set\n",
        "pupil_spon = np.concatenate(pupil_spon, axis=0)\n",
        "pupil_stim = np.concatenate(pupil_stim, axis=0)\n",
        "eeg_spon = np.concatenate(eeg_spon, axis=0)\n",
        "eeg_stim = np.concatenate(eeg_stim, axis=0)\n",
        "\n",
        "# Save to .mat file\n",
        "sio.savemat(os.path.join(save_path, 'pupil_eeg_data_control_dose.mat'), {\n",
        "    'pupil_spon': pupil_spon,\n",
        "    'pupil_stim': pupil_stim,\n",
        "    'eeg_spon': eeg_spon,\n",
        "    'eeg_stim': eeg_stim\n",
        "})\n",
        "\n",
        "print(\"Pupil and EEG data saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GzV0e4CCSk3T"
      },
      "outputs": [],
      "source": [
        "all_data = np.load(os.path.join(save_path, 'all_data.npy'), allow_pickle=True).item()\n",
        "\n",
        "# Define the set of animals\n",
        "set2_animals = ['animal_3398', 'animal_33105', 'animal_31117', 'animal_31108', 'animal_3734']\n",
        "other_animals = [animal for animal in all_data.keys() if animal not in set2_animals]\n",
        "\n",
        "# Function to calculate mean and SEM\n",
        "def calculate_mean_sem(data):\n",
        "    mean_data = np.mean(data, axis=0)\n",
        "    sem_data = np.std(data, axis=0) / np.sqrt(data.shape[0])\n",
        "    return mean_data, sem_data\n",
        "\n",
        "# Iterate over other_animals to plot the average EEG for spontaneous data\n",
        "bands = ['Delta', 'Theta', 'Alpha', 'Beta', 'LowGamma', 'HighGamma']\n",
        "\n",
        "for animal_key in other_animals:\n",
        "    animal_data = all_data[animal_key]\n",
        "    labels = animal_data['label']\n",
        "    eeg_spon = animal_data['eeg'][labels == 0]\n",
        "\n",
        "    # Plot the average EEG for each band\n",
        "    for i, band in enumerate(bands):\n",
        "        eeg_band_data = eeg_spon[:, :, i]\n",
        "        mean_data, sem_data = calculate_mean_sem(eeg_band_data)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(mean_data, label=f'{animal_key} - {band}')\n",
        "        plt.fill_between(range(len(mean_data)), mean_data - sem_data, mean_data + sem_data, alpha=0.3)\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel(f'{band} Power (z-scored)')\n",
        "        plt.title(f'Average {band} Power with SEM - {animal_key}')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "KZJlvPgIHRg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-jMnGYFURQu"
      },
      "outputs": [],
      "source": [
        "# Raw UMAP Visualization\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define file paths\n",
        "prop_file_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/prop_table_aug.csv'\n",
        "phent_file_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/phent_tablenew.csv'\n",
        "control_file_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/control_NEW.csv'\n",
        "clon_file_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/clon_table.csv'\n",
        "\n",
        "# Define function to load and extract data\n",
        "def load_and_extract_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    animals = data['Animal'].unique()\n",
        "    eeg_bands = ['DeltaPwr', 'ThetaPwr', 'AlphaPwr', 'BetaPwr', 'LowGammaPwr', 'HighGammaPwr']\n",
        "    eeg_data = {band: {'stim': [], 'spon': []} for band in eeg_bands}\n",
        "\n",
        "    for animal in animals:\n",
        "        animal_data = data[data['Animal'] == animal]\n",
        "        stim_freq = animal_data['StimulationFrequency'].values\n",
        "        labels = (stim_freq != 0).astype(int)\n",
        "\n",
        "        for band in eeg_bands:\n",
        "            band_data = animal_data[[col for col in animal_data.columns if col.startswith(band)]].values[:, 1:2000]\n",
        "            eeg_data[band]['stim'].append(band_data[labels == 1])\n",
        "            eeg_data[band]['spon'].append(band_data[labels == 0])\n",
        "\n",
        "    # Concatenate all animal data within each condition\n",
        "    for band in eeg_bands:\n",
        "        eeg_data[band]['stim'] = np.concatenate(eeg_data[band]['stim'], axis=0)\n",
        "        eeg_data[band]['spon'] = np.concatenate(eeg_data[band]['spon'], axis=0)\n",
        "\n",
        "    return eeg_data\n",
        "\n",
        "# Load and extract data from each group\n",
        "prop_data = load_and_extract_data(prop_file_path)\n",
        "phent_data = load_and_extract_data(phent_file_path)\n",
        "control_data = load_and_extract_data(control_file_path)\n",
        "clon_data = load_and_extract_data(clon_file_path)\n",
        "\n",
        "# Perform UMAP and plot results\n",
        "def plot_umap_for_band_and_label(eeg_data, label, band, group_names, title):\n",
        "    # Combine data from all groups\n",
        "    combined_data = np.concatenate(\n",
        "        [eeg_data['prop'][band][label], eeg_data['phent'][band][label],eeg_data['clon'][band][label], eeg_data['control'][band][label]], axis=0\n",
        "    )\n",
        "    group_labels = np.concatenate(\n",
        "        [np.full(eeg_data['prop'][band][label].shape[0], group_names[0]),\n",
        "         np.full(eeg_data['phent'][band][label].shape[0], group_names[1]),\n",
        "         np.full(eeg_data['clon'][band][label].shape[0], group_names[2]),\n",
        "         np.full(eeg_data['control'][band][label].shape[0], group_names[3])]\n",
        "    )\n",
        "\n",
        "    # Scale data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "    # Perform UMAP\n",
        "    reducer = umap.UMAP(n_neighbors=75, min_dist=0.1, n_components=2, random_state=42)\n",
        "    embedding = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i, group_name in enumerate(group_names):\n",
        "        plt.scatter(embedding[group_labels == group_name, 0], embedding[group_labels == group_name, 1],\n",
        "                    label=group_name, alpha=0.6)\n",
        "\n",
        "    plt.title(f'UMAP - {title} {band} Band')\n",
        "    plt.xlabel('UMAP 1')\n",
        "    plt.ylabel('UMAP 2')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Define group names\n",
        "group_names = ['Propranolol', 'Phentolamine','Clonidine', 'Control']\n",
        "\n",
        "# Prepare combined data structure\n",
        "eeg_data = {\n",
        "    'prop': prop_data,\n",
        "    'phent': phent_data,\n",
        "    'clon': clon_data,\n",
        "    'control': control_data\n",
        "}\n",
        "\n",
        "# Plot UMAP for each frequency band and condition\n",
        "for band in ['DeltaPwr', 'ThetaPwr', 'AlphaPwr', 'BetaPwr', 'LowGammaPwr', 'HighGammaPwr']:\n",
        "    for label, condition in zip(['stim', 'spon'], ['Stimulated', 'Spontaneous']):\n",
        "        plot_umap_for_band_and_label(eeg_data, label, band, group_names, condition)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to calculate the moving average\n",
        "def moving_average(data, window_size):\n",
        "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
        "\n",
        "# Define function to load and extract data with moving average applied\n",
        "def load_and_extract_data(file_path, window_size=10):\n",
        "    data = pd.read_csv(file_path)\n",
        "    animals = data['Animal'].unique()\n",
        "    eeg_bands = ['DeltaPwr', 'ThetaPwr', 'AlphaPwr', 'BetaPwr', 'LowGammaPwr', 'HighGammaPwr']\n",
        "    eeg_data = {band: {'stim': [], 'spon': []} for band in eeg_bands}\n",
        "\n",
        "    for animal in animals:\n",
        "        animal_data = data[data['Animal'] == animal]\n",
        "        stim_freq = animal_data['StimulationFrequency'].values\n",
        "        labels = (stim_freq != 0).astype(int)\n",
        "\n",
        "        for band in eeg_bands:\n",
        "            band_data = animal_data[[col for col in animal_data.columns if col.startswith(band)]].values[:, 800:1800]\n",
        "            smoothed_data = np.apply_along_axis(moving_average, 1, band_data, window_size)\n",
        "            eeg_data[band]['stim'].append(smoothed_data[labels == 1])\n",
        "            eeg_data[band]['spon'].append(smoothed_data[labels == 0])\n",
        "\n",
        "    # Concatenate all animal data within each condition\n",
        "    for band in eeg_bands:\n",
        "        eeg_data[band]['stim'] = np.concatenate(eeg_data[band]['stim'], axis=0)\n",
        "        eeg_data[band]['spon'] = np.concatenate(eeg_data[band]['spon'], axis=0)\n",
        "\n",
        "    return eeg_data\n",
        "\n",
        "# Load and extract data from each group with smoothing\n",
        "prop_data = load_and_extract_data(prop_file_path)\n",
        "phent_data = load_and_extract_data(phent_file_path)\n",
        "control_data = load_and_extract_data(control_file_path)\n",
        "clon_data = load_and_extract_data(clon_file_path)\n",
        "\n",
        "# Define a function to compute global UMAP embeddings to determine axis limits\n",
        "def compute_global_limits(eeg_data, group_names):\n",
        "    all_embeddings = []\n",
        "    for band in eeg_data[list(eeg_data.keys())[0]].keys():  # Access keys dynamically\n",
        "        for label in ['stim', 'spon']:\n",
        "            combined_data = np.concatenate(\n",
        "                [eeg_data[group][band][label] for group in group_names if group in eeg_data], axis=0\n",
        "            )\n",
        "            scaler = StandardScaler()\n",
        "            scaled_data = scaler.fit_transform(combined_data)\n",
        "            reducer = umap.UMAP(n_neighbors=75, min_dist=0.1, n_components=2, random_state=42)\n",
        "            embedding = reducer.fit_transform(scaled_data)\n",
        "            all_embeddings.append(embedding)\n",
        "\n",
        "    all_embeddings_combined = np.concatenate(all_embeddings, axis=0)\n",
        "    xlim = (all_embeddings_combined[:, 0].min(), all_embeddings_combined[:, 0].max())\n",
        "    ylim = (all_embeddings_combined[:, 1].min(), all_embeddings_combined[:, 1].max())\n",
        "    return xlim, ylim\n",
        "\n",
        "# Calculate global limits for consistent axis ranges\n",
        "xlim, ylim = compute_global_limits({\n",
        "    'prop': prop_data,\n",
        "    'phent': phent_data,\n",
        "    'clon': clon_data,\n",
        "    'control': control_data\n",
        "}, ['prop', 'phent', 'clon', 'control'])\n",
        "\n",
        "# Perform UMAP and plot results with consistent scaling\n",
        "def plot_umap_for_band(eeg_data, band, group_names, title):\n",
        "    # Combine data from all groups for both conditions\n",
        "    stim_data = np.concatenate(\n",
        "        [eeg_data[group][band]['stim'] for group in group_names if group in eeg_data], axis=0\n",
        "    )\n",
        "    spon_data = np.concatenate(\n",
        "        [eeg_data[group][band]['spon'] for group in group_names if group in eeg_data], axis=0\n",
        "    )\n",
        "    combined_data = np.concatenate((stim_data, spon_data), axis=0)\n",
        "\n",
        "    # Create labels for the groups and conditions\n",
        "    stim_labels = np.concatenate(\n",
        "        [np.full(eeg_data[group][band]['stim'].shape[0], f'{group}_stim') for group in group_names if group in eeg_data]\n",
        "    )\n",
        "    spon_labels = np.concatenate(\n",
        "        [np.full(eeg_data[group][band]['spon'].shape[0], f'{group}_spon') for group in group_names if group in eeg_data]\n",
        "    )\n",
        "    group_labels = np.concatenate((stim_labels, spon_labels), axis=0)\n",
        "\n",
        "    # Scale data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "    # Perform UMAP\n",
        "    reducer = umap.UMAP(n_neighbors=75, min_dist=0.1, n_components=2, random_state=42)\n",
        "    embedding = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    markers = {'stim': '^', 'spon': 'o'}  # Define markers for stim and spon\n",
        "    colors = {'prop': 'red', 'phent': 'blue', 'clon': 'green', 'control': 'black'}  # Define colors for groups\n",
        "\n",
        "    for group in group_names:\n",
        "        if group in eeg_data:\n",
        "            for condition in ['stim', 'spon']:\n",
        "                plt.scatter(\n",
        "                    embedding[group_labels == f'{group}_{condition}', 0],\n",
        "                    embedding[group_labels == f'{group}_{condition}', 1],\n",
        "                    label=f'{group} {condition}', alpha=0.6,\n",
        "                    marker=markers[condition], color=colors[group]\n",
        "                )\n",
        "\n",
        "    plt.title(f'UMAP - {title} {band} Band')\n",
        "    plt.xlabel('UMAP 1')\n",
        "    plt.ylabel('UMAP 2')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Define group names\n",
        "group_names = ['prop', 'phent', 'clon', 'control']\n",
        "\n",
        "# Plot UMAP for each frequency band\n",
        "for band in ['DeltaPwr', 'ThetaPwr', 'AlphaPwr', 'BetaPwr', 'LowGammaPwr', 'HighGammaPwr']:\n",
        "    plot_umap_for_band({\n",
        "        'prop': prop_data,\n",
        "        'phent': phent_data,\n",
        "        'clon': clon_data,\n",
        "        'control': control_data\n",
        "    }, band, group_names, 'Combined Stim and Spon')"
      ],
      "metadata": {
        "id": "El7LRMyYbAE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Define file paths\n",
        "prop_file_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/prop_table_aug.csv'\n",
        "phent_file_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/phent_tablenew.csv'\n",
        "control_file_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/control_NEW.csv'\n",
        "clon_file_path = '/content/drive/My Drive/autoencoder/NEW_DATA/CLASSIFY/clon_table.csv'\n",
        "\n",
        "# Function to load and extract data\n",
        "def load_and_extract_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    animals = data['Animal'].unique()\n",
        "    eeg_bands = ['DeltaPwr', 'ThetaPwr', 'AlphaPwr', 'BetaPwr', 'LowGammaPwr', 'HighGammaPwr']\n",
        "    eeg_data = {band: {'stim': [], 'spon': []} for band in eeg_bands}\n",
        "\n",
        "    for animal in animals:\n",
        "        animal_data = data[data['Animal'] == animal]\n",
        "        stim_freq = animal_data['StimulationFrequency'].values\n",
        "        labels = (stim_freq != 0).astype(int)\n",
        "\n",
        "        for band in eeg_bands:\n",
        "            band_data = animal_data[[col for col in animal_data.columns if col.startswith(band)]].values[:, 850:1850]\n",
        "            eeg_data[band]['stim'].append(band_data[labels == 1])\n",
        "            eeg_data[band]['spon'].append(band_data[labels == 0])\n",
        "\n",
        "    # Concatenate all animal data within each condition\n",
        "    for band in eeg_bands:\n",
        "        eeg_data[band]['stim'] = np.concatenate(eeg_data[band]['stim'], axis=0)\n",
        "        eeg_data[band]['spon'] = np.concatenate(eeg_data[band]['spon'], axis=0)\n",
        "\n",
        "    return eeg_data\n",
        "\n",
        "# Load and extract data from each group\n",
        "prop_data = load_and_extract_data(prop_file_path)\n",
        "phent_data = load_and_extract_data(phent_file_path)\n",
        "control_data = load_and_extract_data(control_file_path)\n",
        "clon_data = load_and_extract_data(clon_file_path)\n",
        "\n",
        "# Function to build the classification model with the latent layer output\n",
        "def build_classification_model(input_shape):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "    x = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
        "    conv_output = tf.keras.layers.Conv1D(256, 3, activation='relu', padding='same', name='latent_layer')(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(conv_output)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to train the model and extract latent layer outputs\n",
        "def train_and_extract_latent(eeg_data, band):\n",
        "    latent_outputs = {'prop': {'stim': None, 'spon': None},\n",
        "                      'phent': {'stim': None, 'spon': None},\n",
        "                      'control': {'stim': None, 'spon': None},\n",
        "                      'clon': {'stim': None, 'spon': None}}\n",
        "\n",
        "    group_names = ['prop', 'phent', 'control', 'clon']\n",
        "\n",
        "    for group in group_names:\n",
        "        X_stim = eeg_data[group][band]['stim']\n",
        "        X_spon = eeg_data[group][band]['spon']\n",
        "\n",
        "        # Combine stim and spon data\n",
        "        X = np.concatenate((X_stim, X_spon), axis=0)\n",
        "        y = np.concatenate((np.ones(X_stim.shape[0]), np.zeros(X_spon.shape[0])), axis=0)\n",
        "\n",
        "        # Reshape if the data is 1D\n",
        "        if X.ndim == 2:\n",
        "            X = X[..., np.newaxis]\n",
        "\n",
        "        # KFold cross-validation to train the model\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        latent_vectors_stim = []\n",
        "        latent_vectors_spon = []\n",
        "\n",
        "        for train_index, test_index in kf.split(X):\n",
        "            X_train, X_val = X[train_index], X[test_index]\n",
        "            y_train, y_val = y[train_index], y[test_index]\n",
        "\n",
        "            # Standardize data\n",
        "            scaler = StandardScaler()\n",
        "            X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
        "            X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
        "\n",
        "            # Build and train model\n",
        "            model = build_classification_model((X_train.shape[1], X_train.shape[2]))\n",
        "            early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "            model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0, shuffle=True)\n",
        "\n",
        "            # Extract latent layer output\n",
        "            latent_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('latent_layer').output)\n",
        "            latent_predictions = latent_model.predict(X)\n",
        "\n",
        "            # Flatten the latent outputs to 2D for UMAP\n",
        "            latent_predictions_flat = latent_predictions.reshape(latent_predictions.shape[0], -1)\n",
        "\n",
        "            # Separate stim and spon latent vectors\n",
        "            latent_vectors_stim.append(latent_predictions_flat[:X_stim.shape[0]])\n",
        "            latent_vectors_spon.append(latent_predictions_flat[X_stim.shape[0]:])\n",
        "\n",
        "        # Average latent vectors across folds\n",
        "        latent_outputs[group]['stim'] = np.mean(latent_vectors_stim, axis=0)\n",
        "        latent_outputs[group]['spon'] = np.mean(latent_vectors_spon, axis=0)\n",
        "\n",
        "    return latent_outputs\n",
        "\n",
        "# Perform UMAP and plot results\n",
        "def plot_umap_for_latent(latent_data, group_names, band, condition, xlim, ylim):\n",
        "    # Combine data from all groups for the specified condition\n",
        "    combined_data = np.concatenate(\n",
        "        [latent_data[group][condition] for group in group_names if latent_data[group][condition] is not None], axis=0\n",
        "    )\n",
        "    group_labels = np.concatenate(\n",
        "        [np.full(latent_data[group][condition].shape[0], group) for group in group_names if latent_data[group][condition] is not None]\n",
        "    )\n",
        "\n",
        "    # Scale data\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "    # Perform UMAP\n",
        "    reducer = umap.UMAP(n_neighbors=50, min_dist=0.2, n_components=2, random_state=42)\n",
        "    embedding = reducer.fit_transform(scaled_data)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for group in group_names:\n",
        "        if latent_data[group][condition] is not None:\n",
        "            plt.scatter(embedding[group_labels == group, 0], embedding[group_labels == group, 1],\n",
        "                        label=group, alpha=0.6)\n",
        "\n",
        "    plt.title(f'UMAP - {condition.capitalize()} {band} Band (Latent Space)')\n",
        "    plt.xlabel('UMAP 1')\n",
        "    plt.ylabel('UMAP 2')\n",
        "    plt.xlim(xlim)\n",
        "    plt.ylim(ylim)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Define group names\n",
        "group_names = ['prop', 'phent', 'control', 'clon']\n",
        "\n",
        "# Prepare combined data structure\n",
        "eeg_data = {\n",
        "    'prop': prop_data,\n",
        "    'phent': phent_data,\n",
        "    'control': control_data,\n",
        "    'clon': clon_data\n",
        "}\n",
        "\n",
        "# Calculate global limits for UMAP plots\n",
        "all_embeddings = []\n",
        "for band in ['DeltaPwr', 'ThetaPwr', 'AlphaPwr', 'BetaPwr', 'LowGammaPwr', 'HighGammaPwr']:\n",
        "    latent_outputs = train_and_extract_latent(eeg_data, band)\n",
        "    for condition in ['stim', 'spon']:\n",
        "        # Combine data from all groups\n",
        "        combined_data = np.concatenate(\n",
        "            [latent_outputs[group][condition] for group in group_names if latent_outputs[group][condition] is not None], axis=0\n",
        "        )\n",
        "        scaler = StandardScaler()\n",
        "        scaled_data = scaler.fit_transform(combined_data)\n",
        "        reducer = umap.UMAP(n_neighbors=50, min_dist=0.2, n_components=2, random_state=42)\n",
        "        embedding = reducer.fit_transform(scaled_data)\n",
        "        all_embeddings.append(embedding)\n",
        "\n",
        "# Determine limits based on all embeddings\n",
        "all_embeddings_combined = np.concatenate(all_embeddings, axis=0)\n",
        "xlim = (all_embeddings_combined[:, 0].min(), all_embeddings_combined[:, 0].max())\n",
        "ylim = (all_embeddings_combined[:, 1].min(), all_embeddings_combined[:, 1].max())\n",
        "\n",
        "# Perform UMAP on latent layer outputs for each frequency band and condition\n",
        "for band in ['DeltaPwr', 'ThetaPwr', 'AlphaPwr', 'BetaPwr', 'LowGammaPwr', 'HighGammaPwr']:\n",
        "    print(f\"Processing {band} band...\")\n",
        "    latent_outputs = train_and_extract_latent(eeg_data, band)\n",
        "    for condition in ['stim', 'spon']:\n",
        "        plot_umap_for_latent(latent_outputs, group_names, band, condition, xlim, ylim)"
      ],
      "metadata": {
        "id": "nIAG6rvvHQib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8k3I-Y7dLh3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO2CsgIj2e9uSAdEEfMJ/dx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}